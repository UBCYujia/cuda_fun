{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atomic\n",
    "useful when multiple threads try to write to the same global memory. prevent race condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accumulated result: 1\n"
     ]
    }
   ],
   "source": [
    "function increment_counter(counter)\n",
    "    idx = threadIdx().x + (blockIdx().x - 1) * blockDim().x\n",
    "    if idx <= 100\n",
    "        counter[1] += 1\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "counter = CuArray([0])\n",
    "\n",
    "threads = 100\n",
    "blocks = 1\n",
    "\n",
    "@cuda threads=threads blocks=blocks increment_counter(counter)\n",
    "result = Array(counter)\n",
    "println(\"accumulated result: \", result[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parallel block reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function reduce_atomic(op,a,b)\n",
    "    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x\n",
    "    CUDA.@atomic b[] = op(b[], a[i])\n",
    "    return\n",
    "end\n",
    "\n",
    "a = CuArray(1:16)\n",
    "b = CuArray([0])\n",
    "@cuda threads=length(a) reduce_atomic(+, a, b)\n",
    "CUDA.@allowscalar b[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced sum result: 136\n"
     ]
    }
   ],
   "source": [
    "function reduce_block(op, a, b)\n",
    "    #each thread takes care of 2 elements\n",
    "    elements = blockDim().x * 2 \n",
    "    # @cuprintf(\"Elements: %d\\n\", elements)\n",
    "    thread = threadIdx().x \n",
    "\n",
    "    d = 1 #distance between elements\n",
    "    while d < elements\n",
    "        sync_threads() \n",
    "        index = 2 * d * (thread-1) + 1\n",
    "        if index <= elements && index+d <= length(a)\n",
    "            @inbounds a[index] = op(a[index], a[index+d])\n",
    "        end\n",
    "        d *= 2\n",
    "    end\n",
    "\n",
    "    if thread == 1\n",
    "        b[1] = a[1]\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "a = CuArray(1:16)  \n",
    "b = CuArray([0])   \n",
    "\n",
    "threads = cld(length(a),2)  \n",
    "@cuda threads=threads reduce_block(+, a, b) \n",
    "\n",
    "result = Array(b)\n",
    "println(\"reduced sum result: \", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's fine to have loops in gpu code but make sure every iteration the kernel function behaves identically   \n",
    "gpu performs the best when threads are executing the same operation.  \n",
    "bad case --> thread divergence  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shared memory\n",
    "access to shared memory between threads is faster than access to global memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function reduce_grid_atomic_shmem(op, a, b)\n",
    "    elements = blockDim().x * 2\n",
    "    thread = threadIdx().x\n",
    "    block = blockIdx().x\n",
    "    offset = (block-1) * elements\n",
    "\n",
    "\n",
    "    shared = @cuStaticSharedMem(Int32, (2048,))\n",
    "    @inbounds shared[thread+blockDim().x] = a[offset+thread+blockDim().x]\n",
    "    @inbounds shared[thread] = a[offset+thread]\n",
    "\n",
    "    d = 1\n",
    "    while d < elements\n",
    "        sync_threads()\n",
    "        index = 2 * d * (thread-1) + 1\n",
    "        @inbounds if index < elements && offset+index+d <= length(a)\n",
    "            shared[index] = op(shared[index], shared[index+d])\n",
    "        end\n",
    "        d *= 2\n",
    "    end\n",
    "\n",
    "    if thread == 1\n",
    "        CUDA.@atomic b[] = op(b[], shared[1])\n",
    "    end\n",
    "\n",
    "    return\n",
    "end\n",
    "\n",
    "a = CuArray(1:16) \n",
    "b = CuArray([0]) \n",
    "\n",
    "@cuda threads=4 blocks=2 reduce_grid_atomic_shmem(+, a, b)\n",
    "CUDA.@allowscalar b[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
